<!DOCTYPE html>
<html lang="en-us" style="font-size: 120%">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <meta name="author" content="">

  
  
  <meta name="description" content="just a scratch pad for problems
2. regularization from the augmentation perspective

Let us consider a particular OLS setup: there is a data matrix $X\in\mathbb{R}^{n\times d}$, a measurement vector $y\in\mathbb{R}^{n}$, a weight vector $w\in\mathbb{R}^{n}$ such that $w\sim \mathcal{N}(0_{d}, \Sigma)$, $\Sigma=(\Gamma^{\intercal}\Gamma)^{-1}$ is positive semi-definite, and a model
$$ y_i=w^{\intercal}x_i &#43; \epsilon ,\qquad \epsilon\sim\mathcal{N}(0,1).$$
Suppose we augment the data matrix and measurement vector so
$$ \hat{X} = \begin{pmatrix} X \\ \Gamma\end{pmatrix}\in\mathbb{R}^{(n&#43;d)\times d},\qquad \hat{y}=\begin{pmatrix} y \\ 0_{d} \end{pmatrix}\in\mathbb{R}^{n&#43;d}.$$
Show that the OLS solution with these augmented components is equal to the maximum a posteriori (MAP) estimate by Tikhonov regularization:
$$ \mathrm{argmin}_{w\in\mathbb{R}^{n}} \lVert \hat{y} - \hat{X}w \rVert_{2}^{2} = \left(X^{\intercal}X &#43; \Sigma^{-1}\right)^{-1}X^{\intercal}y.$$">
  

  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">


  
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
<script type="module">
    import renderMathInElement from "https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.mjs";
    renderMathInElement(document.body);
</script>
  

  
  <meta property="og:url" content="http://localhost:1313/dead/deepneuralnetworks/">
  <meta property="og:site_name" content="mindful math">
  <meta property="og:title" content="mindful math">
  <meta property="og:description" content="just a scratch pad for problems 2. regularization from the augmentation perspective Let us consider a particular OLS setup: there is a data matrix $X\in\mathbb{R}^{n\times d}$, a measurement vector $y\in\mathbb{R}^{n}$, a weight vector $w\in\mathbb{R}^{n}$ such that $w\sim \mathcal{N}(0_{d}, \Sigma)$, $\Sigma=(\Gamma^{\intercal}\Gamma)^{-1}$ is positive semi-definite, and a model $$ y_i=w^{\intercal}x_i &#43; \epsilon ,\qquad \epsilon\sim\mathcal{N}(0,1).$$ Suppose we augment the data matrix and measurement vector so $$ \hat{X} = \begin{pmatrix} X \\ \Gamma\end{pmatrix}\in\mathbb{R}^{(n&#43;d)\times d},\qquad \hat{y}=\begin{pmatrix} y \\ 0_{d} \end{pmatrix}\in\mathbb{R}^{n&#43;d}.$$ Show that the OLS solution with these augmented components is equal to the maximum a posteriori (MAP) estimate by Tikhonov regularization: $$ \mathrm{argmin}_{w\in\mathbb{R}^{n}} \lVert \hat{y} - \hat{X}w \rVert_{2}^{2} = \left(X^{\intercal}X &#43; \Sigma^{-1}\right)^{-1}X^{\intercal}y.$$">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="dead">
    <meta property="article:modified_time" content="2023-09-17T22:30:00-04:00">


  
  <link rel="canonical" href="http://localhost:1313/dead/deepneuralnetworks/">

  
  
  
  <meta itemprop="name" content="mindful math">
  <meta itemprop="description" content="just a scratch pad for problems 2. regularization from the augmentation perspective Let us consider a particular OLS setup: there is a data matrix $X\in\mathbb{R}^{n\times d}$, a measurement vector $y\in\mathbb{R}^{n}$, a weight vector $w\in\mathbb{R}^{n}$ such that $w\sim \mathcal{N}(0_{d}, \Sigma)$, $\Sigma=(\Gamma^{\intercal}\Gamma)^{-1}$ is positive semi-definite, and a model $$ y_i=w^{\intercal}x_i &#43; \epsilon ,\qquad \epsilon\sim\mathcal{N}(0,1).$$ Suppose we augment the data matrix and measurement vector so $$ \hat{X} = \begin{pmatrix} X \\ \Gamma\end{pmatrix}\in\mathbb{R}^{(n&#43;d)\times d},\qquad \hat{y}=\begin{pmatrix} y \\ 0_{d} \end{pmatrix}\in\mathbb{R}^{n&#43;d}.$$ Show that the OLS solution with these augmented components is equal to the maximum a posteriori (MAP) estimate by Tikhonov regularization: $$ \mathrm{argmin}_{w\in\mathbb{R}^{n}} \lVert \hat{y} - \hat{X}w \rVert_{2}^{2} = \left(X^{\intercal}X &#43; \Sigma^{-1}\right)^{-1}X^{\intercal}y.$$">
  <meta itemprop="dateModified" content="2023-09-17T22:30:00-04:00">
  <meta itemprop="wordCount" content="614">

  
  <link media="screen" rel="stylesheet" href='http://localhost:1313/css/common.css'>
  <link media="screen" rel="stylesheet" href='http://localhost:1313/css/content.css'>

  
  
  <title> - mindful math</title>
  

  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="mindful math">
  <meta name="twitter:description" content="just a scratch pad for problems 2. regularization from the augmentation perspective Let us consider a particular OLS setup: there is a data matrix $X\in\mathbb{R}^{n\times d}$, a measurement vector $y\in\mathbb{R}^{n}$, a weight vector $w\in\mathbb{R}^{n}$ such that $w\sim \mathcal{N}(0_{d}, \Sigma)$, $\Sigma=(\Gamma^{\intercal}\Gamma)^{-1}$ is positive semi-definite, and a model $$ y_i=w^{\intercal}x_i &#43; \epsilon ,\qquad \epsilon\sim\mathcal{N}(0,1).$$ Suppose we augment the data matrix and measurement vector so $$ \hat{X} = \begin{pmatrix} X \\ \Gamma\end{pmatrix}\in\mathbb{R}^{(n&#43;d)\times d},\qquad \hat{y}=\begin{pmatrix} y \\ 0_{d} \end{pmatrix}\in\mathbb{R}^{n&#43;d}.$$ Show that the OLS solution with these augmented components is equal to the maximum a posteriori (MAP) estimate by Tikhonov regularization: $$ \mathrm{argmin}_{w\in\mathbb{R}^{n}} \lVert \hat{y} - \hat{X}w \rVert_{2}^{2} = \left(X^{\intercal}X &#43; \Sigma^{-1}\right)^{-1}X^{\intercal}y.$$">


  
<link rel="stylesheet" href='http://localhost:1313/css/single.css'>
<link rel="stylesheet" href='http://localhost:1313/css/sharingbuttons.css'>

</head>

<body>
  <div id="wrapper">
    



      <script async src="https://www.googletagmanager.com/gtag/js?id=G-9G7EGTLBT9"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-9G7EGTLBT9');
        }
      </script>
<header id="header">
  <h1>
    <a href="http://localhost:1313/">mindful math</a>
  </h1>

  <nav>
    
    <span class="nav-bar-item">
      <a class="link" href="/">posts</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/post/">archive</a>
    </span>
    
    <span class="nav-bar-item">
      <a class="link" href="/about/">about</a>
    </span>
    
  </nav>

  

  <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>
  
</header>
    
<main id="main" class="post">
  
  <article class="content ">
    <h2 id="just-a-scratch-pad-for-problems">just a scratch pad for problems</h2>
<h3 id="2-regularization-from-the-augmentation-perspective">2. regularization from the augmentation perspective</h3>
<blockquote>
<p>Let us consider a particular OLS setup: there is a data matrix $X\in\mathbb{R}^{n\times d}$, a measurement vector $y\in\mathbb{R}^{n}$, a weight vector $w\in\mathbb{R}^{n}$ such that $w\sim \mathcal{N}(0_{d}, \Sigma)$, $\Sigma=(\Gamma^{\intercal}\Gamma)^{-1}$ is positive semi-definite, and a model
$$ y_i=w^{\intercal}x_i + \epsilon ,\qquad \epsilon\sim\mathcal{N}(0,1).$$
Suppose we augment the data matrix and measurement vector so
$$ \hat{X} = \begin{pmatrix} X \\ \Gamma\end{pmatrix}\in\mathbb{R}^{(n+d)\times d},\qquad \hat{y}=\begin{pmatrix} y \\ 0_{d} \end{pmatrix}\in\mathbb{R}^{n+d}.$$
Show that the OLS solution with these augmented components is equal to the maximum a posteriori (MAP) estimate by Tikhonov regularization:
$$ \mathrm{argmin}_{w\in\mathbb{R}^{n}} \lVert \hat{y} - \hat{X}w \rVert_{2}^{2} = \left(X^{\intercal}X + \Sigma^{-1}\right)^{-1}X^{\intercal}y.$$</p>
</blockquote>
<p>This is a lame problem. We use algebra to get from the OLS solution of the left-hand side to get to the right:</p>
<p>\begin{align}
(\hat{X}^{\intercal}\hat{X})^{-1}\hat{X}^{\intercal}\hat{y} &amp;= \begin{bmatrix} \begin{pmatrix} X^{\intercal} &amp; \Gamma^{\intercal} \end{pmatrix} &amp; \begin{pmatrix} X \\ \Gamma \end{pmatrix} \end{bmatrix}^{-1} \begin{pmatrix} X^{\intercal} &amp; \Gamma^{\intercal} \end{pmatrix} \begin{pmatrix} y \\ 0_{d} \end{pmatrix} \\
&amp;= ( X^{\intercal}X + \Gamma^{\intercal}\Gamma )^{-1} X^{\intercal}y
\end{align}</p>
<h3 id="3-vector-calculus-review">3. vector calculus review</h3>
<blockquote>
<p>Suppose $x,c\in\mathbb{R}^{n}$, $A=(a_1,\dots, a_n)\in\mathbb{R}^{n\times n}$, and $\nabla_x f(x)=\langle \frac{\partial f}{\partial x_1},\dots,\frac{\partial f}{\partial x_n}\rangle$ is a row vector. Then show</p>
<ol>
<li>$\nabla_x (x^{\intercal} c) = c^{\intercal}$</li>
<li>$\nabla_x \lVert x\rVert_2^2=2x^{\intercal}$</li>
<li>$\nabla_x (Ax) = A$</li>
<li>$\nabla_x (x^{\intercal} A x)=x^{\intercal}(A+A^{\intercal})$</li>
<li>Under what condition is 4. equal to $2x^{\intercal}A$?</li>
</ol>
</blockquote>
<ol>
<li>$\nabla_x (x^{\intercal} c) = \nabla_x \left(x_1c_1 + x_2c_2 + \cdots + x_nc_n\right)  = c^{\intercal}$</li>
<li>$\nabla_x \lVert x\rVert_2^2 = \nabla_x x^{\intercal}x = \nabla_x \left( x_1^2 + x_2^2 + \cdots + x_n^2\right) = 2x^{\intercal}$</li>
<li>$\nabla_x (Ax) = \nabla_x (a_1x_1 + a_2x_2 + \cdots a_nx_n) = A$</li>
<li>For some reason, KaTeX doesn&rsquo;t support align without numbering:
\begin{align} \nabla_x (x^{\intercal} A x) &amp;= \nabla_x \sum_{i=1}^n\sum_{j=1}^n a_{ij}x_ix_j \\ &amp;= \left\langle \sum_{i=1}^n a_{i1}x_i + \sum_{j=1}^n a_{1j}x_j,\dots, \sum_{i=1}^n a_{in}x_i + \sum_{j=1}^n a_{nj}x_j\right\rangle \\ &amp;=x^{\intercal}(A+A^{\intercal}) \end{align}</li>
<li>When $A$ is symmetric, $x^{\intercal}(A+A^{\intercal})=2x^{\intercal}A$</li>
</ol>
<h3 id="4-relu-elbow-update-under-sgd">4. relu elbow update under sgd</h3>
<blockquote>
<ol>
<li>Find the location of the elbow - i.e. where the loss transitions from zero to something else.</li>
<li>Find the derivative of the loss with respect to $\phi(x)$.</li>
<li>Find the partial derivative of the loss with respect to $w$.</li>
<li>Find the partial derivative of the loss with resepct to $b$.</li>
<li>What happens to the slope and elbow of $\phi(x)$ when $\phi(x)=0$.</li>
<li>What happens to the slope and elbow of $\phi(x)$ when $w&gt;0,x&gt;0,$ and $\phi(x)&gt;0$.</li>
<li>What happens to the slope and elbow of $\phi(x)$ when $w&gt;0,x&lt;0,$ and $\phi(x)&gt;0$.</li>
<li>What happens to the slope and elbow of $\phi(x)$ when $w&lt;0,x&gt;0,$ and $\phi(x)&gt;0$.</li>
</ol>
</blockquote>
<ol>
<li>$x=-b/w$ is where the &ldquo;elbow&rdquo; (the non-differentiable point of the loss) is located.</li>
<li>$$\frac{d\ell}{d\phi}=(\phi(x)-y)\phi&rsquo;(x)=\begin{cases} (\phi(x)-y)w &amp; wx + b &gt; 0 \\ 0 &amp; \text{otherwise} \end{cases}$$</li>
<li>$$ \frac{\partial\ell}{\partial w}= \begin{cases} (\phi(x)-y)x &amp; wx + b &gt; 0 \\ 0 &amp; \text{otherwise} \end{cases}$$</li>
<li>$$ \frac{\partial\ell}{\partial b}= \begin{cases} \phi(x)-y &amp; wx + b &gt; 0 \\ 0 &amp; \text{otherwise} \end{cases}$$</li>
<li>We have
$$ \begin{bmatrix} w_{t+1} \\ b_{t+1} \end{bmatrix}\leftarrow \begin{bmatrix} w_{t} \\ b_{t} \end{bmatrix} - \eta(\phi_t - y)\begin{bmatrix} x\phi_t \\ \phi_t \end{bmatrix}=\begin{bmatrix} w_{t} \\ b_{t} \end{bmatrix}$$</li>
</ol>
<p>and so the slope, $w_{t+1}$, and elbow, $-\frac{b_{t+1}}{w_{t+1}}$, stay the same as before.</p>
<ol start="6">
<li>$$ \begin{bmatrix} w_{t+1} \\ b_{t+1} \end{bmatrix}\leftarrow \begin{bmatrix} w_{t} \\ b_{t} \end{bmatrix} - \eta(\phi_t - y)\begin{bmatrix} x\phi_t \\ \phi_t \end{bmatrix}$$</li>
</ol>
<p>and so the slope, $w_{t+1}$, decreases by a positive amount $\eta x\phi_t$. The elbow becomes</p>
<p>$$ -\frac{b_{t}-\eta\phi_{t}}{w_{t}-\eta x\phi_{t}}=\underbrace{\frac{-b_t}{w_t}}_{\text{prior elbow}} \times\frac{1-\frac{\eta\phi_{t}}{b_{t}}}{1-\frac{\eta x\phi_{t}}{w_t}}$$</p>
<p>which moves right or left depending on exact values of $\phi_t, x$, and $w_t$.</p>
<ol start="7">
<li>
<p>It&rsquo;s the same update as above, but the slope increases by $\eta x\phi_t$ as $x$ is negative here. However the elbow is slightly different - here $\phi_t&gt;0$ but $x&lt;0$ implies that $b_t&gt;w_tx$ and so the elbow will move towards the origin.</p>
</li>
<li>
<p>The slope decreases by a positive amount $\eta x\phi_t$ while the elbow again will scale but it depends exactly on $w_t,b_t,x$, and $\phi_t$.</p>
</li>
</ol>

  </article>

  

</main>

    <footer id="footer">
  <div>
    <span>© 2023</span> - <span>2024</span>
  </div>

  <div class="footnote">
    <span></span>
  </div>
</footer>

  </div>

  
  

  
  

  
  
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-9G7EGTLBT9"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-9G7EGTLBT9');
        }
      </script>

</body>

</html>
